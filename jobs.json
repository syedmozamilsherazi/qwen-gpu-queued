{
  "queue": [
    {
      "id": "08e82cbf-9b49-4feb-9ce3-2d6d2f554fd0",
      "messages": [
        {
          "content": "You are a systematic literature review (SLR) screening assistant.\nYour job is to BE INCLUSIVE - we must NOT miss any relevant papers about AI/ML for eye diseases.\n\n\nDOMAIN: AI/ML/DL applications in ophthalmology and eye diseases.\n\nRELEVANT EYE DISEASES (SELECT if ANY mentioned):\n- Diabetic Retinopathy (DR)\n- Glaucoma\n- Cataract\n- Age-related Macular Degeneration (AMD/ARMD)\n- Retinal disorders (retinopathy, retinal detachment, macular edema, etc.)\n- Ophthalmic conditions (keratoconus, dry eye, conjunctivitis, etc.)\n- Retinal vessel diseases\n- Optic nerve disorders\n- Corneal diseases\n- ANY eye/vision/ocular/ophthalmic disease or condition\n\nRELEVANT TASKS (SELECT if ANY mentioned):\n- Diagnosis / Detection / Identification\n- Classification / Grading / Staging\n- Prediction / Prognosis\n- Segmentation (pixel-based, semantic, instance, etc.)\n- Screening / Triage\n- Image Analysis / Feature Extraction\n- Lesion Detection / Localization\n- Automated / Computer-aided analysis\n\nRELEVANT AI/ML METHODS (SELECT if ANY mentioned):\n- Artificial Intelligence (AI)\n- Machine Learning (ML)\n- Deep Learning (DL)\n- Neural Networks (CNN, RNN, LSTM, Transformers, U-Net, ResNet, VGG, etc.)\n- Computer Vision\n- Image Processing / Analysis\n- Segmentation (pixel-based, semantic, watershed, region-based, etc.)\n- Feature Extraction / Selection\n- Pattern Recognition\n- Automated / Computerized methods\n- Transfer Learning\n- Ensemble methods\n- Support Vector Machines (SVM)\n- Random Forest\n- K-Nearest Neighbors\n- Convolutional methods\n- Attention mechanisms\n- ANY computational/algorithmic approach to medical image analysis\n\n\n================================================================================\nTASK: Evaluate this paper ABSTRACT for inclusion.\n(This paper already passed title screening - it likely IS relevant)\n================================================================================\n\nTITLE: Attention Driven Multimodal Fusion: Application of Manual Templates in Feature aggregation\nABSTRACT: In agricultural production, pest problems are inevitable. In recent years, China has lost up to 40 million tons of grain annually due to various pests and diseases. In nature, with over one million known insect species, traditional manual identification is costly, inefficient and difficult to accurately distinguish all pests. With the development of computer technology, vision-based unimodal recognition models have been proposed. Due to the complex background of pest habitats and the diverse morphologies of pests, unimodal recognition models often lead to issues such as insufficient identification accuracy and low data utilization rates. As smart agriculture advances, multimodal data continues to emerge. Therefore, current research is focused on cross-modal data fusion to compensate for the deficiencies of unimodal recognition. However, research at this stage tends to overlook the importance of cross-modal hierarchical features and issues such as redundancy in textual features. In order to solve the above problems, this paper proposes a multiscale intermodal feature fusion model (ITFNet), which fuses multiscale visual and textual features for intermodal attention learning, enabling the model to effectively utilize the complementary nature of multimodal data. Specifically, pest text descriptions are designed to aggregate manual template features in feature extraction via language model (LM) to obtain text features; meanwhile, pest image data are used to obtain visual features at different levels via a five-layer cascaded downsampling module. To enhance the model's attention to key regions, the extracted visual features are fused with textual features after being processed by an attention mechanism. Compared to the traditional unimodal YOLOv4 model, the model proposed in this paper achieves an 18.02\\% improvement in the mAP (mean Average Precision) metric. In addition, in order to prove the uniqueness of high-level features in cross-modality, this study also conducts ablation experiments on different levels of visual and textual features, and the results show that the model's mAP value has a 2.38\\% improvement compared to the full fusion after the exclusion of high-level visual feature fusion, which further proves the importance of the selective fusion strategy. In summary, the multiscale cross-modal feature fusion model proposed in this paper has practical application value for advancing the development of multimodal modeling in the field of agriculture, enhancing the depth and breadth of agricultural pest control research, and even improving the efficiency of agricultural production.\n\n================================================================================\nSELECTION RULES - BE VERY INCLUSIVE:\n================================================================================\n\nOUTPUT: Abstract_Selected\nIF ANY of the following is TRUE:\n- Abstract mentions ANY eye disease (diabetic retinopathy, glaucoma, cataract, AMD, retinal, macular, etc.)\n- Abstract mentions ANY AI/ML technique (neural network, deep learning, segmentation, classification, detection, CNN, etc.)\n- Abstract describes automated/computerized analysis of eye images\n- Abstract mentions fundus images, OCT scans, retinal images, or ophthalmic imaging\n- Abstract describes segmentation, feature extraction, or pixel-based analysis for eye disease\n- Abstract discusses diagnosis, detection, classification, grading, or prediction of eye conditions\n- Abstract mentions dataset of eye/retinal/fundus images\n- Abstract is about computer-aided diagnosis for ophthalmology\n- Abstract is UNCLEAR but the title suggests relevance\n- Abstract is MISSING (select by default)\n\nOUTPUT: Abstract_Deleted\nONLY IF the abstract CLEARLY AND EXPLICITLY confirms ALL of these:\n- This is ONLY a review/survey/meta-analysis paper (not original research) AND\n- NO AI/ML/DL methodology is actually used or proposed AND\n- The focus is NOT on eye diseases at all\n\n================================================================================\nCRITICAL EXAMPLES:\n- \"Pixel-based segmentation for diabetic retinopathy\" = Abstract_Selected\n  (Segmentation IS an AI/image processing technique!)\n- \"Automated identification using feature extraction\" = Abstract_Selected\n  (Automated + feature extraction = computational/AI approach!)\n- \"Deep learning for retinal vessel segmentation\" = Abstract_Selected\n- \"CNN-based classification of fundus images\" = Abstract_Selected\n================================================================================\n\nOUTPUT (one word only):\nAbstract_Selected\nOR\nAbstract_Deleted\n\nYOUR RESPONSE:",
          "role": "user"
        }
      ],
      "max_tokens": 256
    }
  ]
}