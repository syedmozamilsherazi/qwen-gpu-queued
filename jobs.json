{
  "queue": [
    {
      "id": "f95e5870-19f8-4792-8e9e-08fba5486385",
      "messages": [
        {
          "content": "You are a systematic literature review (SLR) screening assistant.\nYour job is to BE INCLUSIVE - we must NOT miss any relevant papers about AI/ML for eye diseases.\n\n\nDOMAIN: AI/ML/DL applications in ophthalmology and eye diseases.\n\nRELEVANT EYE DISEASES (SELECT if ANY mentioned):\n- Diabetic Retinopathy (DR)\n- Glaucoma\n- Cataract\n- Age-related Macular Degeneration (AMD/ARMD)\n- Retinal disorders (retinopathy, retinal detachment, macular edema, etc.)\n- Ophthalmic conditions (keratoconus, dry eye, conjunctivitis, etc.)\n- Retinal vessel diseases\n- Optic nerve disorders\n- Corneal diseases\n- ANY eye/vision/ocular/ophthalmic disease or condition\n\nRELEVANT TASKS (SELECT if ANY mentioned):\n- Diagnosis / Detection / Identification\n- Classification / Grading / Staging\n- Prediction / Prognosis\n- Segmentation (pixel-based, semantic, instance, etc.)\n- Screening / Triage\n- Image Analysis / Feature Extraction\n- Lesion Detection / Localization\n- Automated / Computer-aided analysis\n\nRELEVANT AI/ML METHODS (SELECT if ANY mentioned):\n- Artificial Intelligence (AI)\n- Machine Learning (ML)\n- Deep Learning (DL)\n- Neural Networks (CNN, RNN, LSTM, Transformers, U-Net, ResNet, VGG, etc.)\n- Computer Vision\n- Image Processing / Analysis\n- Segmentation (pixel-based, semantic, watershed, region-based, etc.)\n- Feature Extraction / Selection\n- Pattern Recognition\n- Automated / Computerized methods\n- Transfer Learning\n- Ensemble methods\n- Support Vector Machines (SVM)\n- Random Forest\n- K-Nearest Neighbors\n- Convolutional methods\n- Attention mechanisms\n- ANY computational/algorithmic approach to medical image analysis\n\n\n================================================================================\nTASK: Evaluate this paper ABSTRACT for inclusion.\n(This paper already passed title screening - it likely IS relevant)\n================================================================================\n\nTITLE: Bio-sensing of Environmental Distress for Walkable Built Environment\nABSTRACT: The negative environmental stimuli (e.g., poorly maintained sidewalks, blighted properties, graffiti, trash on the ground, unsafe traffic conditions) in the urban built environment are linked to stress symptomatology in a significant portion of the urban populations. It plays a significant contributor to the increase of urban-associated diseases such as depression, allergies, asthma, diabetes, and cardiovascular diseases. A few studies presented the potential to identify pedestrians' environmental distress caused by the negative environmental stimuli using bio-signals (e.g., gait patterns, blood volume pulse, and electrodermal activity) beyond the subjectivity concerns of traditional approaches such as neighborhood surveys and field observation. However, there remain several unanswered questions regarding whether the effect of the negative environmental stimuli can be identified from bio-signals captured in naturalistic ambulatory settings, which include various uncontrollable confounding factors (e.g., movement artifacts, physiology reactivity due to non-intended stimuli, and individual variability). In this context, this talk discusses the challenges and opportunities of leveraging bio-signals in capturing environmental distress. We examine empirical associations between bio-signals and environmental stimuli commonly observed in neighborhood-built environment. Then we present a novel method that identifies group-level environmental distress by capturing and aggregating prominent local patterns of bio-signals from multiple pedestrians. In addition, the potential benefits of multimodal data are illustrated through the experimental results that predict environmental distress by using both bio-signals and image-based data (e.g., visual features captured from built environment image, such as land use, sidewalk connectivity, and road speeds).\n\n================================================================================\nSELECTION RULES - BE VERY INCLUSIVE:\n================================================================================\n\nOUTPUT: Abstract_Selected\nIF ANY of the following is TRUE:\n- Abstract mentions ANY eye disease (diabetic retinopathy, glaucoma, cataract, AMD, retinal, macular, etc.)\n- Abstract mentions ANY AI/ML technique (neural network, deep learning, segmentation, classification, detection, CNN, etc.)\n- Abstract describes automated/computerized analysis of eye images\n- Abstract mentions fundus images, OCT scans, retinal images, or ophthalmic imaging\n- Abstract describes segmentation, feature extraction, or pixel-based analysis for eye disease\n- Abstract discusses diagnosis, detection, classification, grading, or prediction of eye conditions\n- Abstract mentions dataset of eye/retinal/fundus images\n- Abstract is about computer-aided diagnosis for ophthalmology\n- Abstract is UNCLEAR but the title suggests relevance\n- Abstract is MISSING (select by default)\n\nOUTPUT: Abstract_Deleted\nONLY IF the abstract CLEARLY AND EXPLICITLY confirms ALL of these:\n- This is ONLY a review/survey/meta-analysis paper (not original research) AND\n- NO AI/ML/DL methodology is actually used or proposed AND\n- The focus is NOT on eye diseases at all\n\n================================================================================\nCRITICAL EXAMPLES:\n- \"Pixel-based segmentation for diabetic retinopathy\" = Abstract_Selected\n  (Segmentation IS an AI/image processing technique!)\n- \"Automated identification using feature extraction\" = Abstract_Selected\n  (Automated + feature extraction = computational/AI approach!)\n- \"Deep learning for retinal vessel segmentation\" = Abstract_Selected\n- \"CNN-based classification of fundus images\" = Abstract_Selected\n================================================================================\n\nOUTPUT (one word only):\nAbstract_Selected\nOR\nAbstract_Deleted\n\nYOUR RESPONSE:",
          "role": "user"
        }
      ],
      "max_tokens": 256
    }
  ]
}